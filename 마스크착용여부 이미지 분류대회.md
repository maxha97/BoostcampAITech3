# 마스크 착용여부 이미지 분류 대회(Level1)



## 진행하면서 학습하고 시도해본 것들

### Loss
- CrossEntropy Loss   
![image](https://user-images.githubusercontent.com/53209003/156934799-f7ad4c0c-5f38-46af-9342-16a008444f26.png)   
엔트로피 개념이 손실(loss)함수에서 사용된 사례로 볼 수 있음. 만약 이진분류 태스크가 아닌 N개의 라벨 중 하나로 분류하는 태스크일 때, 입력 데이터가 모델을 통과할 경우 소프트맥스 함수에 의해 각 클래스에 속할 확률이 계산됨(Q(x)). 실제 값의 경우, 정답인 클래스만 확률이 1이고, 나머지는 모두 0인 확률분포에 해당(P(x)). 이 두 확률분포의 차이를 Cross Entropy 수식 형태로 표현하여 계산할 수 있음. Cross Entropy를 줄이기 위한 파라미터를 구하는 것이 negative log likelohood를 최소화하는 파라미터를 구하는 것과 동일

- Focal Loss   
분류 태스크에 있어서 기존의 Cross Entropy에  가중치를 곱함으로써, Easy Negative 샘플에 대한 학습에 중점적으로 뒀던 학습과정에 패널티를 부여함으로써, 데이터 라벨 비중이 낮은Hard Negative 샘플에 대해서도 추가적으로 학습할 수 있는 구조를 취하여 불균형 데이터에 효과적임을 알 수 있었다. 

- F1 Loss




## 추후 시도해보면 좋은 것들
-	**Nivida APEX – AMP**   
엔비디아에서 제공중이며, 현재는 파이토치 기본 라이브러리로 탑재된 APEX 패키지는  AMP(Automatic Mixed Precision) 기능을 제공하고 있음. AMP 기능을 활용할 경우 FP16연산과 FP32 연산을 섞어 학습을 진행할 수 있음. 이렇게 진행하게 될 경우, 학습 속도에 있어 향상이 있으며, 16bit를 혼용함에 따라 램사용량을 최적화할 수 있음. 이에 따라, 기존에는 네이버 V100 서버에서의 90기가램에서 설정하지 못한 배치사이즈를 시도해보거나, 더 많은 파라미터 수를 가진 모델(ex. Efficientnet_b7)을 시도해 봄으로써 성능 향상을 기대해볼 수 있을 것.

-	**Pytorch Lightning**   
파이토치의 하이레벨 언어로 모델학습 및 예측 과정에서 훨씬 가벼운 코드를 작성할 수 있도록 도와줌. 추후, 코드 작성에 소모되는 시간을 단축하기 위해 시도해보면 좋을 것으로 생각됨
([파이토치] 머신러닝 Pytorch 모델의 성능을 극대화하는 7가지 팁!: https://bbdata.tistory.com/9)
